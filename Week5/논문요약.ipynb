{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPOOG3mZdRgE7ym9VrRWG8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jun0S2/AIBootcamp/blob/main/%EB%85%BC%EB%AC%B8%EC%9A%94%EC%95%BD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain chromadb openai tiktoken pypdf langchain-community langchain-chroma langchain-openai bs4 requests"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J7hi1h05mRc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain\n",
        "!pip install langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "n8sSw4k8nTEz",
        "outputId": "0dc9baee-2093-4d9f-b600-688d48549817"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 설치 목록\n",
        "\n",
        "* langchain: LangChain의 기본 라이브러리\n",
        "* langchain-community: PyPDFLoader 같은 커뮤니티 모듈을 제공\n",
        "* langchain-chroma: ChromaDB를 LangChain과 함께 사용하는 데 필요한 라이브러리\n",
        "* langchain-openai: OpenAI와 LangChain을 통합하기 위한 패키지\n",
        "* bs4: BeautifulSoup4를 사용해 HTML 파싱을 수행\n",
        "* requests: PDF를 다운로드하는 데 필요"
      ],
      "metadata": {
        "id": "psbAMEL_mYjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QxZ77BVTWIDN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.schema import Document\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_paper(pdf_url, prompt):\n",
        "    \"\"\"\n",
        "    Summarizes a research paper given its PDF URL and a prompt.\n",
        "\n",
        "    Args:\n",
        "        pdf_url (str): URL of the research paper in PDF format.\n",
        "        prompt (str): Prompt to guide the LLM for summarization.\n",
        "\n",
        "    Returns:\n",
        "        str: Summarization result from the LLM.\n",
        "    \"\"\"\n",
        "    # Step 1: Download the PDF\n",
        "    pdf_path = \"paper.pdf\"\n",
        "    response = requests.get(pdf_url)\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Step 2: Load the PDF as documents\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    raw_docs = loader.load()\n",
        "\n",
        "    # Step 3: Split the documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    split_docs = text_splitter.split_documents(raw_docs)\n",
        "\n",
        "    # Step 4: Store the chunks in a vectorstore\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=split_docs,\n",
        "        embedding=OpenAIEmbeddings(api_key=\"...\")\n",
        "    )\n",
        "\n",
        "    # Step 5: Use retriever to find relevant chunks\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    retrieved_docs = retriever.invoke(prompt)\n",
        "\n",
        "    # Step 6: Format the retrieved documents\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    context = format_docs(retrieved_docs)\n",
        "\n",
        "    # Step 7: Use LLM to summarize\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        api_key=\"...\"\n",
        "    )\n",
        "    llm_prompt = f\"{prompt}\\n\\nContext:\\n{context}\"\n",
        "    response = llm.invoke(llm_prompt)\n",
        "\n",
        "    # Clean up downloaded PDF\n",
        "    os.remove(pdf_path)\n",
        "\n",
        "    return response.content\n"
      ],
      "metadata": {
        "id": "qGlC1s-bl9r5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "s2Gq6h7loSRl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "pdf_url = \"https://arxiv.org/pdf/2005.11401\"\n",
        "prompt = \"논문의 주요 연구 목표, 방법론, 그리고 결론을 요약해줘.\"\n",
        "summary = summarize_paper(pdf_url, prompt)\n",
        "print(\"Summary:\\n\", summary)"
      ],
      "metadata": {
        "id": "OV8ZLtoWl8KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65da6970-33bb-4680-e9b1-a0ee497ba26c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " 해당 논문 \"Coarse-to-fine question answering for long documents\"의 주요 연구 목표, 방법론, 결론을 요약하면 다음과 같습니다.\n",
            "\n",
            "### 주요 연구 목표\n",
            "이 연구의 주요 목표는 긴 문서에서 효과적으로 질문에 대한 답변을 찾는 방법을 제안하는 것입니다. 특히, 기존의 QA (Question Answering) 시스템들이 긴 문서에서 신뢰할 수 있는 정보에 접근하는 데 어려움을 겪는 문제를 해결하고자 했습니다.\n",
            "\n",
            "### 방법론\n",
            "1. **Coarse-to-Fine Strategy**: 연구진은 질문에 대한 답변을 찾기 위해 두 단계의 접근 방식을 사용했습니다. 첫 번째 단계는 문서에서 질문과 관련된 중요한 정보 조각을 식별하는 'Coarse' 단계이며, 두 번째 단계는 이 정보를 바탕으로 더 정교한 'Fine' 단계를 통해 최종 답변을 생성하는 방식입니다.\n",
            "  \n",
            "2. **Retrieval-Augmented Generation (RAG)**: 이 시스템은 정보 검색을 통합하여 모델이 이전에 학습한 내용 외에도 새로운 정보를 활용할 수 있게 합니다.\n",
            "\n",
            "3. **Baseline Comparisons**: 연구진은 제안한 방법을 기존의 state-of-the-art 시스템과 비교하여 성능을 평가했습니다.\n",
            "\n",
            "### 결론\n",
            "연구 결과, 제안된 coarse-to-fine 접근 방식이 긴 문서 내에서의 질문 응답 문제를 효과적으로 해결할 수 있음을 보여주었습니다. 해당 방법은 기존의 복잡한 파이프라인 시스템과 비교하여 단순하고 효율적이면서도 경쟁력 있는 성능을 발휘하여, 긴 문서에서의 QA 작업에서 유망한 가능성을 나타냈습니다. 연구 결과는 복잡한 처리 과정을 줄이면서도 높은 정확도를 유지할 수 있는 새로운 접근 방식의 필요성을 강조합니다.\n",
            "\n",
            "이 요약은 논문의 주요 목표, 방법론 및 결론을 간략하게 정리한 것입니다. 추가적인 상세 내용은 논문의 원문을 참조하시기 바랍니다.\n"
          ]
        }
      ]
    }
  ]
}
